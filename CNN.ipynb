{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import progressbar\n",
    "import matplotlib.pyplot as pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from urllib.request import urlopen\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# For reproducibility, set the initial seed\n",
    "seed = 7\n",
    "np.random.seed(7)\n",
    "\n",
    "def load_dataset(dataset_fp, delimiter=\",\",chunksize=1000):\n",
    "    if not os.path.isfile(dataset_fp):\n",
    "        response = urlopen(\"http://cs.mcgill.ca/~ksinha4/datasets/kaggle/\" + dataset_fp)\n",
    "        CHUNK = 16 * chunksize\n",
    "        with open(dataset_fp, 'wb') as f:\n",
    "            while True:\n",
    "                chunk = response.read(CHUNK)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                f.write(chunk)\n",
    "    \n",
    "    \n",
    "    chunks = []\n",
    "    pb = progressbar.ProgressBar()\n",
    "    for chunk in pb(pd.read_csv(dataset_fp, delimiter=delimiter, chunksize=chunksize, header=None)):\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "    dataset = pd.concat(chunks)\n",
    "    return dataset.as_matrix()\n",
    "\n",
    "\n",
    "def train_validation_set_split(trainset_x, trainset_y, **kwargs):\n",
    "    trainset = np.concatenate((trainset_x, trainset_y),axis=1)\n",
    "    return train_test_split(trainset, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage \n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import closing, square\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def get_regions(image):\n",
    "    bw = closing(image > 0.99, square(1))\n",
    "    \n",
    "    # label image regions\n",
    "    label_image = label(bw)\n",
    "    return [region.image for region in regionprops(label_image)]\n",
    "\n",
    "def max_region_by_area(regions):\n",
    "    return max(regions, key = lambda x : max(x.shape[0] * x.shape[0], x.shape[1] * x.shape[1]))\n",
    "\n",
    "\n",
    "def to_squre(region):\n",
    "    (w, h) = region.shape\n",
    "    desired_size = 32\n",
    "    delta_w = desired_size - h\n",
    "    delta_h = desired_size - w\n",
    "    padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
    "    im = Image.fromarray(region.astype('uint8')*255)\n",
    "    new_im = ImageOps.expand(im, padding)\n",
    "    im_array = np.array(new_im)\n",
    "    transformed_im = skimage.transform.resize(im_array, (desired_size,desired_size))\n",
    "    return transformed_im\n",
    "    \n",
    "    \n",
    "def preprocess_image(image):\n",
    "    p_image = image.reshape(64,64)\n",
    "    p_image = p_image.astype('float32')\n",
    "    regions = get_regions(p_image)\n",
    "    max_area_region = max_region_by_area(regions)\n",
    "    return to_squre(max_area_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrainload = load_dataset(\"train_x.csv\")\n",
    "ytrainload = load_dataset(\"train_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = xtrainload / 255.0\n",
    "ytrain = ytrainload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = progressbar.ProgressBar()\n",
    "\n",
    "\n",
    "# preprocess x \n",
    "xtrain_preprocessed = []\n",
    "for x in pb(xtrain):\n",
    "    result = preprocess_image(x)\n",
    "    result = result.reshape(1024)\n",
    "    xtrain_preprocessed.append(result)\n",
    "    \n",
    "# Preprocess y \n",
    "ytrain = np_utils.to_categorical(ytrain)\n",
    "xtrain = np.asarray(xtrain_preprocessed)\n",
    "num_classes = ytrain.shape[1]\n",
    "\n",
    "\n",
    "xtrainset = xtrain[:-10000]\n",
    "ytrainset = ytrain[:-10000]\n",
    "xvalidset = xtrain[-10000:]\n",
    "yvalidset = ytrain[-10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_cnn():\n",
    "    '''\n",
    "    ConvNet1\n",
    "    Following online tutorial found here:\n",
    "    https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\n",
    "    '''\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6, (5, 5), input_shape=(1, 32, 32), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def lecun_cnn():\n",
    "    '''\n",
    "    ConvNet2\n",
    "    Parameter choices inspired by the LeCun-5 paper which was optimized for performance on\n",
    "    MNIST dataset.\n",
    "    http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6, (5, 5), input_shape=(1, 32, 32), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(16, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(180, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(180, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def lecun_cnn_double_hidden_layers():\n",
    "    '''\n",
    "    ConvNet3\n",
    "    Additional hidden layer was added \n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6, (5, 5), input_shape=(1, 32, 32), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(16, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(180, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(180, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(180, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(180, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def lecun_cnn_double_hidden_layer_width():\n",
    "    '''\n",
    "    ConvNet4\n",
    "    Parameter choices inspired by the LeCun-5 paper which was optimized for performance on\n",
    "    MNIST dataset.\n",
    "    http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6, (5, 5), input_shape=(1, 32, 32), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(16, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(360, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(360, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def lecun_cnn_double_convolution():\n",
    "    '''\n",
    "    ConvNet5\n",
    "    '''\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6, (5, 5), input_shape=(1, 32, 32), activation='relu'))\n",
    "    model.add(Conv2D(6, (5, 5), input_shape=(1, 32, 32), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(16, (5, 5), activation='relu'))\n",
    "    model.add(Conv2D(16, (5, 5), input_shape=(1, 32, 32), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(180, activation='relu'))\n",
    "    model.add(Dense(180, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def lecun_cnn_double_convolution_wider():\n",
    "    '''\n",
    "    ConvNet6\n",
    "    '''\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(1, 32, 32), activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainxall = np.append(xtrainset, xvalidset)\n",
    "trainyall = np.append(ytrainset, yvalidset)\n",
    "trainxallshaped = trainxall.reshape(xtrainload.shape[0],1,32,32)\n",
    "trainyallshaped = trainyall.reshape(ytrainload.shape[0],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Base CNN from tutorial\n",
    "'''\n",
    "base_model = base_cnn()\n",
    "base_model.fit(xtrainset.reshape(xtrainset.shape[0],1,32,32),\n",
    "          ytrainset, \n",
    "          validation_data=(xvalidset.reshape(xvalidset.shape[0],1,32,32), yvalidset),\n",
    "          batch_size=200, \n",
    "          epochs=20, \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LeCun-5 Inspired cnn weights.\n",
    "'''\n",
    "\n",
    "lecun_model = lecun_cnn()\n",
    "lecun_model.fit(xtrainset.reshape(xtrainset.shape[0],1,32,32),\n",
    "          ytrainset, \n",
    "          validation_data=(xvalidset.reshape(xvalidset.shape[0],1,32,32), yvalidset),\n",
    "          batch_size=200, \n",
    "          epochs=20, \n",
    "          verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Doubling Number of Hidden Layers\n",
    "'''\n",
    "lecun_model_2 = lecun_cnn_double_hidden_layers()\n",
    "lecun_model_2.fit(xtrainset.reshape(xtrainset.shape[0],1,32,32),\n",
    "          ytrainset, \n",
    "          validation_data=(xvalidset.reshape(xvalidset.shape[0],1,32,32), yvalidset),\n",
    "          batch_size=200, \n",
    "          epochs=20, \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Doubling Width of Hidden Layers\n",
    "'''\n",
    "lecun_model_3 = lecun_cnn_double_hidden_layers()\n",
    "lecun_model_3.fit(xtrainset.reshape(xtrainset.shape[0],1,32,32),\n",
    "          ytrainset, \n",
    "          validation_data=(xvalidset.reshape(xvalidset.shape[0],1,32,32), yvalidset),\n",
    "          batch_size=200, \n",
    "          epochs=20, \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Doubling Convolution Layers and Width\n",
    "'''\n",
    "lecun_model_4 = lecun_cnn_double_convolution()\n",
    "lecun_model_4.fit(xtrainset.reshape(xtrainset.shape[0],1,32,32),\n",
    "          ytrainset, \n",
    "          validation_data=(xvalidset.reshape(xvalidset.shape[0],1,32,32), yvalidset),\n",
    "          batch_size=200, \n",
    "          epochs=20, \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Increase width and number of convolutional layers, increase width of dense layers.\n",
    "'''\n",
    "\n",
    "lecun_model_5 = lecun_cnn_double_convolution_wider()\n",
    "lecun_model_5.fit(xtrainset.reshape(xtrainset.shape[0],1,32,32),\n",
    "          ytrainset, \n",
    "          validation_data=(xvalidset.reshape(xvalidset.shape[0],1,32,32), yvalidset),\n",
    "          batch_size=200, \n",
    "          epochs=20, \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtestset = load_dataset(\"test_x.csv\")\n",
    "xtestset = xtestset / 255.\n",
    "\n",
    "pb = progressbar.ProgressBar()\n",
    "\n",
    "xtest_preprocessed = []\n",
    "for x in pb(xtestset):\n",
    "    result = preprocess_image(x)\n",
    "    result = result.reshape(1024)\n",
    "    xtest_preprocessed.append(result)\n",
    "\n",
    "xtestset = np.asarray(xtest_preprocessed)\n",
    "xtestset = xtestset.reshape(xtestset.shape[0],1,32,32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model, save results to upload to Kaggle. \n",
    "\n",
    "lecun_model = lecun_cnn_double_convolution_wider()\n",
    "lecun_model.fit(trainxallshaped,\n",
    "                trainyallshaped, \n",
    "                batch_size=200, \n",
    "                epochs=25, \n",
    "                verbose=2)\n",
    "\n",
    "\n",
    "predictions = lecun_model.predict(xtestset)\n",
    "\n",
    "with open('results.csv','w') as prediction_file:\n",
    "    prediction_file.write(\"Id,Label\\n\")\n",
    "    for i,prediction in enumerate(predictions):\n",
    "        prediction_file.write(\"%d,%d\\n\"%(i,list(prediction).index(max(prediction))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Create diagrams of each CNN.\n",
    "\n",
    "cnns = [base_cnn, \n",
    "        lecun_cnn, \n",
    "        lecun_cnn_double_convolution, \n",
    "        lecun_cnn_double_convolution_wider, \n",
    "        lecun_cnn_double_hidden_layer_width,\n",
    "        lecun_cnn_double_hidden_layers]\n",
    "\n",
    "for cnn in cnns:\n",
    "    file_name = cnn.__name__ + \".png\"\n",
    "    plot_model(cnn(), to_file=file_name)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
