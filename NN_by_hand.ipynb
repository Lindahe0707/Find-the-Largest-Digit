{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import progressbar\n",
    "import os\n",
    "import matplotlib.pyplot as pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold \n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "# For reproducibility, set the initial seed\n",
    "seed = 7\n",
    "np.random.seed(7)\n",
    "def load_dataset(dataset_fp, delimiter=\",\",chunksize=1000):\n",
    "    if not os.path.isfile(dataset_fp):\n",
    "        response = urlopen(\"http://cs.mcgill.ca/~ksinha4/datasets/kaggle/\" + dataset_fp)\n",
    "        CHUNK = 16 * chunksize\n",
    "        with open(dataset_fp, 'wb') as f:\n",
    "            while True:\n",
    "                chunk = response.read(CHUNK)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                f.write(chunk)\n",
    "    \n",
    "    \n",
    "    chunks = []\n",
    "    pb = progressbar.ProgressBar()\n",
    "    for chunk in pb(pd.read_csv(dataset_fp, delimiter=delimiter, chunksize=chunksize, header=None)):\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "    dataset = pd.concat(chunks)\n",
    "    return dataset.as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing some preprocessing \n",
    "\n",
    "import math\n",
    "import skimage \n",
    "import cv2\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import closing, square\n",
    "from skimage.util import pad\n",
    "from skimage.transform import warp, AffineTransform\n",
    "from scipy import ndimage\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def get_regions_otsu_method(image):\n",
    "    bw = closing(image > 0.99, square(1))\n",
    "    \n",
    "    # label image regions\n",
    "    label_image = label(bw)\n",
    "    return [region.image for region in regionprops(label_image)]\n",
    "\n",
    "def max_region_by_area(regions):\n",
    "    return max(regions, key = lambda x : max(x.shape[0] * x.shape[0], x.shape[1] * x.shape[1]))\n",
    "\n",
    "\n",
    "def to_squre(region):\n",
    "    #convert rectangular image to square, keeping the ratio\n",
    "    (h, w) = region.shape\n",
    "    desired_size = 32\n",
    "    delta_w = desired_size - w\n",
    "    delta_h = desired_size - h\n",
    "    padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
    "    im = Image.fromarray(region.astype('uint8')*255)\n",
    "    new_im = ImageOps.expand(im, padding)\n",
    "    im_array = np.array(new_im)\n",
    "    transformed_im = skimage.transform.resize(im_array, (desired_size,desired_size))\n",
    "    return transformed_im\n",
    "\n",
    "def preprocess_image(image):\n",
    "    p_image = image.reshape(64,64)\n",
    "    p_image = p_image.astype('float32')\n",
    "    regions = get_regions_otsu_method(p_image)\n",
    "    max_area_region = max_region_by_area(regions)\n",
    "    return to_squre(max_area_region)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self,size, learning_rate, epoch):\n",
    "\n",
    "        self.input_size = 1024\n",
    "        self.hidden_size = size\n",
    "        self.output_size = 10\n",
    "        self.lr = learning_rate\n",
    "        self.epo = epoch\n",
    "\n",
    "        #weights\n",
    "        self.weight_hidden = np.random.randn(self.input_size, self.hidden_size)* np.sqrt(2.0/self.input_size) # (1024*hidden_size) weight matrix from input to hidden layer\n",
    "        self.weight_output = np.random.randn(self.hidden_size, self.output_size)* np.sqrt(2.0/self.hidden_size) # (hidden_size*10) weight matrix from hidden to output layer\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # activation function\n",
    "        z = np.clip( z, -500, 500 )\n",
    "        return 1.0 /(1.0 + np.exp(-z))\n",
    "\n",
    "    def sigmoidPrime(self, z):\n",
    "        #derivative of sigmoid\n",
    "        return z*(1- z)\n",
    "\n",
    "    def forward(self, X):\n",
    "        #forward propagation through our network\n",
    "        self.z = np.dot(X, self.weight_hidden) \n",
    "        self.act_hidden = self.sigmoid(self.z) # hidden layer activation function\n",
    "        self.act_output = np.dot(self.act_hidden, self.weight_output) \n",
    "        output = self.sigmoid(self.act_output) # output activation function\n",
    "        #output = self.softmax(self.act_output) \n",
    "        return output\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        learning_rate = 0.1\n",
    "        self.error_output = y - output \n",
    "        self.delta_output = self.error_output*self.sigmoidPrime(output) \n",
    "\n",
    "        self.error_hidden = self.delta_output.dot(self.weight_output.T) \n",
    "        self.delta_hidden = self.error_hidden*self.sigmoidPrime(self.act_hidden) \n",
    "        \n",
    "        self.weight_hidden += self.lr*X.reshape(self.input_size,1).dot(self.delta_hidden.reshape(1, self.hidden_size)) # update weight in hidden layer\n",
    "        self.weight_output += self.lr*self.act_hidden.reshape(self.hidden_size, 1).dot(self.delta_output.reshape(1, self.output_size)) #update wight in output layer\n",
    "        \n",
    "    def gradient(self, X, y):\n",
    "        #do gradient descent on sample\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output)\n",
    "                       \n",
    "    def evaluate(self, X, original_y):\n",
    "        # y is an array with single digit\n",
    "        output = self.forward(X)\n",
    "        sum_y = 0\n",
    "        size = X.shape[0]\n",
    "        for i in range(X.shape[0]):\n",
    "            predict = np.argmax(output[i])\n",
    "            if predict == original_y[i][0]:\n",
    "                sum_y += 1         \n",
    "        return  1.0*sum_y/size\n",
    "\n",
    "        \n",
    "    def predict(self, xtrainset, ytrainset, xvalidset, original_y):\n",
    "        print \"epoch\",\n",
    "        for j in range(self.epo):\n",
    "            print j,\n",
    "            for i in range(xtrainset.shape[0]):\n",
    "                self.gradient(xtrainset[i],ytrainset[i])\n",
    "#             print(\"the {}/12 epoch:\".format(j),self.evaluate(xvalidset, original_y))\n",
    "        return self.evaluate(xvalidset, original_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrainload = load_dataset(\"train_x.csv\")\n",
    "ytrainload = load_dataset(\"train_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = progressbar.ProgressBar()\n",
    "\n",
    "# preprocess x \n",
    "xtrain = xtrainload / 255.0\n",
    "xtrain_preprocessed = []\n",
    "for x in pb(xtrain):\n",
    "    result = preprocess_image(x)\n",
    "    result = result.reshape(1024)\n",
    "    xtrain_preprocessed.append(result)\n",
    "xtrain = np.asarray(xtrain_preprocessed)\n",
    "\n",
    "# Preprocess y \n",
    "def one_hot(y):\n",
    "    size = y.shape[0]\n",
    "    y_new = np.empty((size,10))\n",
    "    for i in range(size):\n",
    "        y_one_hot = np.zeros(10)\n",
    "        index = int(y[i][0])\n",
    "        y_one_hot[index] = 1\n",
    "        y_new[i] = y_one_hot\n",
    "    return y_new\n",
    "ytrain = one_hot(ytrainload)\n",
    "\n",
    "#use 5 folds cross validation\n",
    "kf = KFold(n_splits=5,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hidden node = 100, learning rate = 0.1\n",
    "record = []\n",
    "i = 1\n",
    "for train_index, val_index in kf.split(xtrain):\n",
    "    print \"creating NN\"\n",
    "    net1 = Neural_Network(300, 0.1, 12)\n",
    "    print \"spliting samples...\"\n",
    "    xtrainset, xvalidset = xtrain[train_index], xtrain[val_index]\n",
    "    ytrainset, yvalidset = ytrain[train_index], ytrainload[val_index] # use orginal y digit array\n",
    "    print \"Training model with fold \" + str(i) + \"...\"\n",
    "    result = net1.predict(xtrainset, ytrainset, xvalidset, yvalidset)\n",
    "    print(\"accuracy for fold {}:\".format(i), result)\n",
    "    record.append(result)\n",
    "    i+=1\n",
    "print(\"mean accuracy = \", np.mean(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hidden node = 100, learning rate = 0.1\n",
    "record = []\n",
    "i = 1\n",
    "for train_index, val_index in kf.split(xtrain):\n",
    "    print \"creating NN\"\n",
    "    net1 = Neural_Network(500, 0.1, 12)\n",
    "    print \"spliting samples...\"\n",
    "    xtrainset, xvalidset = xtrain[train_index], xtrain[val_index]\n",
    "    ytrainset, yvalidset = ytrain[train_index], ytrainload[val_index] # use orginal y digit array\n",
    "    print \"Training model with fold \" + str(i) + \"...\"\n",
    "    result = net1.predict(xtrainset, ytrainset, xvalidset, yvalidset)\n",
    "    print(\"accuracy for fold {}:\".format(i), result)\n",
    "    record.append(result)\n",
    "    i+=1\n",
    "print(\"mean accuracy = \", np.mean(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden node = 64, learning rate = 0.1\n",
    "\n",
    "record = []\n",
    "i = 1\n",
    "for train_index, val_index in kf.split(xtrain):\n",
    "\n",
    "    net1 = Neural_Network(300, 0.07, 12)\n",
    "    xtrainset, xvalidset = xtrain[train_index], xtrain[val_index]\n",
    "    ytrainset, yvalidset = ytrain[train_index], ytrainload[val_index] # use orginal y digit array\n",
    "    result = net1.predict(xtrainset, ytrainset, xvalidset, yvalidset)\n",
    "    print(\"accuracy for fold {}:\".format(i), result)\n",
    "    record.append(result)\n",
    "    i+=1\n",
    "print(\"mean accuracy = \", np.mean(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden node = 300, learning rate = 0.1\n",
    "\n",
    "record = []\n",
    "i = 1\n",
    "for train_index, val_index in kf.split(xtrain):\n",
    "\n",
    "    net3 = Neural_Network(300, 0.085, 12)\n",
    "    xtrainset, xvalidset = xtrain[train_index], xtrain[val_index]\n",
    "    ytrainset, yvalidset = ytrain[train_index], ytrainload[val_index] # use orginal y digit array\n",
    "    result = net3.predict(xtrainset, ytrainset, xvalidset, yvalidset)\n",
    "    print(\"accuracy for fold {}:\".format(i), result)\n",
    "    record.append(result)\n",
    "    i+=1\n",
    "print(\"mean accuracy = \", np.mean(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hidden node = 600, learning rate = 0.1\n",
    "\n",
    "record = []\n",
    "i = 1\n",
    "for train_index, val_index in kf.split(xtrain):\n",
    "\n",
    "    net4 = Neural_Network(400, 0.1, 12)\n",
    "    xtrainset, xvalidset = xtrain[train_index], xtrain[val_index]\n",
    "    ytrainset, yvalidset = ytrain[train_index], ytrainload[val_index] # use orginal y digit array\n",
    "    result = net4.predict(xtrainset, ytrainset, xvalidset, yvalidset)\n",
    "    print(\"accuracy for fold {}:\".format(i), result)\n",
    "    record.append(result)\n",
    "    i+=1\n",
    "print(\"mean accuracy = \", np.mean(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden node = 300, learning rate = 0.5\n",
    "\n",
    "record = []\n",
    "i = 1\n",
    "for train_index, val_index in kf.split(xtrain):\n",
    "\n",
    "    net5 = Neural_Network(300, 0.5, 12)\n",
    "    xtrainset, xvalidset = xtrain[train_index], xtrain[val_index]\n",
    "    ytrainset, yvalidset = ytrain[train_index], ytrainload[val_index] # use orginal y digit array\n",
    "    result = net5.predict(xtrainset, ytrainset, xvalidset, yvalidset)\n",
    "    print(\"accuracy for fold {}:\".format(i), result)\n",
    "    record.append(result)\n",
    "    i+=1\n",
    "print(\"mean accuracy = \", np.mean(record))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
